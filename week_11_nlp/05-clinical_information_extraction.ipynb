{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.visualization import visualize_dep, visualize_ent\n",
    "from medspacy.context import ConTextItem\n",
    "from medspacy.ner import TargetRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Clinical Information Extraction\n",
    "Over the last two weeks, you've been introduced to a number of tools for extracting information from clinical text:\n",
    "- A rule-based matcher using the `TargetMatcher` class\n",
    "- A pre-trained statistical `NER` model for extracting **\"PROBLEM\"**, **\"TREATMENT\"**, and **\"TEST\"** entities\n",
    "- `ConTextComponent` for extracting contextual information such as negation, uncertainty, and family history\n",
    "\n",
    "For your homework assignment, we'll put it all together, improve our model, and deploy it on MIMIC data. Here is an outline of this assignment:\n",
    "\n",
    "- Build an medspaCy model which includes the `TargetMatcher`, statistical `NER`, and `ConTextComponent`\n",
    "- Load a sample of discharge summaries from MIMIC\n",
    "- Review the output of your NLP model on a small number of datasets and make imnprovements by adding patterns or ConTextItems\n",
    "- Deploy your NLP model on the entire dataset and convert it to structured data\n",
    "- Analyze the classes and spans of text extracted by your model\n",
    "\n",
    "As usual, let me know on Slack or Canvas if you have any questions or issues. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Build your model\n",
    "We'll create a new model by loading the various pieces which we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "Load a clinical `nlp` model using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = medspacy.load(\"en_info_3700_i2b2_2012\", \n",
    "                    disable=[\"tagger\", \"parser\"] # Disable the POS tagger and dependency parser to speed up performance\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the two components that we will customize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher = nlp.get_pipe(\"target_matcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = nlp.get_pipe(\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Get Discharge Summaries MIMIC Data\n",
    "A **discharge summary** is written at the end of a patient's stay in the hospital. It typically contains a summary of the patient, the diagnoses for which they were admitted, and the treatment that they received during their stay. The rich content of these documents makes them an excellent candidate for processing with NLP.\n",
    "\n",
    "Clinical documents are stored in MIMIC in the table `noteevents`. We will query a number of notes from this table and limit them to discharge summaries through the **\"category\"** column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your username\n",
    "username = \"\"\n",
    "\n",
    "conn = pymysql.connect(host=\"35.233.174.193\",port=3306,\n",
    "                       user=username,passwd=getpass.getpass(\"Enter password for MIMIC2 database\"),\n",
    "                       db='mimic2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT subject_id, text\n",
    "FROM noteevents\n",
    "WHERE category = 'DISCHARGE_SUMMARY'\n",
    "ORDER BY RAND()\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Process your texts and review the output\n",
    "Next, we'll process the discharge summaries and review what our system extracts. Processing full notes is a computationally expensive process, so we'll start by looking at just a few texts before processing the entire batch later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "texts = df[\"text\"].iloc[:5] # Small sample to start with\n",
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.visualization import visualize_ent, visualize_dep\n",
    "from medspacy.visualization import MedspaCyVisualizerWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = MedspaCyVisualizerWidget(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "# visualize_ent(docs[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Improve your model\n",
    "As we've seen, our default model is not going to be perfect. If you'd like to spend some time improving your model, go through a few docs above and find mistakes. Then fix them using the methods we saw in previous notebooks.\n",
    "\n",
    "- **False negatives**: Missing a target entity. This will happen when you see a clinical problem, treatment or test in the text that is not highlighted. You can fix this by **adding patterns** to the `ruler`\n",
    "- **False positives**: Spans of text which are highlighted but should not be. These are harder to fix. You could write rules to remove an entity from `doc.ents`, but this is a little tricky and difficult to generalize\n",
    "- **Missing modifiers**: ConText modifiers, such as **\"NEGATED_EXISTENCE\"** will be highlighted in the text as well. If you see one that is missing, add it to ConText by creating a new `ConTextItem`. You can also visualize what targets the modifiers are applied to by using the `visualize_dep` function.\n",
    "    - **A note about `visualize_dep`**: This function works best on a *single* sentence rather than an entire doc. So instead of calling `visualize_dep(doc)`, manually add some text, process it with the nlp, and then view the output by calling:  `visualize_dep(nlp(\"...\"))`\n",
    "    \n",
    "Edit the cells below to add `TargetRules` and `ConTextItems` to fix mistakes you find in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.ner import TargetRule\n",
    "from medspacy.context import ConTextItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher = nlp.get_pipe(\"target_matcher\")\n",
    "\n",
    "target_rules = [\n",
    "    # TargetRule(...),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher.add(target_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = nlp.get_pipe(\"context\")\n",
    "\n",
    "context_rules = [\n",
    "    # ConTextItem(...)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.add(context_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've added new rules, go back to the cells at the beginning of this section, reprocess your docs, and reload your visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now go back, reprocess the doc, and see if your changes worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deploy your model and convert text to structured data\n",
    "Now that you've fine-tuned and improved your model, we're ready to run it on the entire dataset and analyze it! In this step, we'll show how you can use NLP to convert text to **structured** data, which you can then analyze in the same way that we previously analyzed structured EHR data like **labs** and **vitals**. We'll now extract all of the entities from our docs and convert them into a pandas DataFrame.\n",
    "\n",
    "Start by creating a list called `docs` which contains all the `doc` objects created by our model. We can do this by calling `nlp.pipe()` on the column of the DataFrame containing the text notes and then converting it to a list. This might take a minute or two. We'll measure how long it takes by using the `%%time` magic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "texts = df[\"text\"] # Process all of the texts\n",
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll add the processed `docs` to our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"doc\"] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to structured data\n",
    "One of the primary tasks of NLP is to take **unstructured, free-text data** and convert it to **structured data** which can be analyzed using methods which we've done previously. In this next section, we'll convert all of the entities in our docs into a DataFrame. \n",
    "\n",
    "Below is a helper function which will take our DataFrame with one row per document and return a new DataFrame with one row per entity, along with attributes of the entities as columns. Here are the attributes we want to save for each ent:\n",
    "\n",
    "- `subject_id`: The patient identifier so that we can do patient-level analysis. This is stored in the **\"subject_id\"** column of `df`\n",
    "- `text`: The text which is included in span. To normalize all of these phrases, we'll lowercase it by accessing the `ent.lower_` attribute\n",
    "- `sent`: The text of the sentence containing the entity. This will be helpful later if we want to look at the context of an entity. This is available as a string in the attribute `ent.sent.text`\n",
    "- `label`: The label assigned by our NER model or entity ruler. You can access this through the `ent.label_` attribute\n",
    "- `is_negated`, `is_historical`, `is_uncertain`, `is_family`, and `is_hypothetical`: Each of the attributes extracted by cycontext. We will use this later to analyze what conditions occur in a patient's family history or by excluding conditions which were never experienced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ents_df(df):\n",
    "    ent_dicts = []\n",
    "    for i, row in df.iterrows():\n",
    "        ent_dicts += process_row(row)\n",
    "    return pd.DataFrame(ent_dicts)\n",
    "        \n",
    "def process_row(row):\n",
    "    ent_dicts = []\n",
    "    for ent in row[\"doc\"].ents:\n",
    "        ent_dicts.append(ent_to_dict(row[\"subject_id\"], ent))\n",
    "    return ent_dicts\n",
    "        \n",
    "def ent_to_dict(subject_id, ent):\n",
    "    d = {}\n",
    "    d[\"subject_id\"] = subject_id\n",
    "    d[\"text\"] = ent.lower_\n",
    "    d[\"label\"] = ent.label_\n",
    "    d[\"sent\"] = ent.sent.text\n",
    "    \n",
    "    # ConText attributes\n",
    "    d[\"is_negated\"] = ent._.is_negated\n",
    "    d[\"is_historical\"] = ent._.is_historical\n",
    "    d[\"is_uncertain\"] = ent._.is_uncertain\n",
    "    d[\"is_family\"] = ent._.is_family\n",
    "    d[\"is_hypothetical\"] = ent._.is_hypothetical\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to generate the new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_df = create_ents_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analysis\n",
    "Now, we can analyze our extracted dataset using pandas and matplotlib. Go through each of the sections below and follow the instructions to analyze the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Label distribution\n",
    "Let's see how any **problems**, **treatments**, and **tests** are extracted. Plot the count of entity labels in the dataset. Generate a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_df.____(\"label\").size().____.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Treatment texts\n",
    "Let's see what treatments are being used in these patient visits.\n",
    "\n",
    "### TODO\n",
    "- Using boolean indexing, create a DataFrame called `treatments` which contains only **TREATMENT** entities\n",
    "- Then identify the 10 most common texts in that DataFrame by calling `treatments[\"text\"].value_counts().head(10)`. \n",
    "    - This is similar to `treatments.groupby(\"text\").size()`, but it will sort it and select the 10 most frequent. Save the output of this as `common_treatment_texts`. \n",
    "- Then, plot a horizontal bar graph of `common_treatment_texts`. (Horizontal because that will make the labels easier to read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = ents_df[ents_df[\"label\"] == ____]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_treatment_texts = treatments[\"text\"].value_counts().head(10)\n",
    "common_treatment_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "____.____.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Problems relevant to a visit\n",
    "As we saw in the previous notebook, many of the conditions mentioned in a document were not actually experienced by a patient during the hospital stay. That is why we ran **context** to generate the attributes such as **is_negated**. Let's now look at all problems in the dataset which are **relevant** to the dataset, meaning that all of the context attributes are `False` (ie., the problem is **not** historical, **not** negated, etc.)\n",
    "\n",
    "### TODO\n",
    "- Using boolean indexing, creating a new DataFrame called `problems` where the **label** is **\"PROBLEM\"** \n",
    "- Next, filter the rows to show those where all of the ConText attributes are False. \n",
    "- Save this as a DataFrame called `relv_problems` .\n",
    "- Plot the 10 most frequent spans of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relv_problems = problems[\n",
    "    (problems[\"is_negated\"] == False)\n",
    "        &\n",
    "    (problems[\"is_historical\"] == False)\n",
    "        &\n",
    "    (problems[\"is_uncertain\"] == False)\n",
    "        &\n",
    "    (problems[\"is_family\"] == False)\n",
    "        &\n",
    "    (problems[\"is_hypothetical\"] == False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relv_problems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relv_problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the 10 most common spans of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "____[____].value_counts().head(10).____.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Patients with a family history of cancer\n",
    "In addition to **excluding** conditions which are not experienced by a patient, context can also help us target conditions which occurred in a patient's family history. While these conditions may not directly affect a patient, they are important to a patient's health because they might suffer from a heightened risk for this condition or other complications.\n",
    "\n",
    "In cycontext, we can detect this by using the `is_family` attribute. \n",
    "\n",
    "**Note:** In cycontext, modifiers like **\"girlfriend\"** or **\"husband\"** are also considered **\"FAMILY\"**. In a real analysis of a patient's family history you would restrict the lexicon to a smaller number of modifiers which are actually family members.\n",
    "\n",
    "Let's now find patients with family history of cancer and see what types of cancer they have.\n",
    "\n",
    "### TODO\n",
    "- Using boolean indexing, creating a new DataFrame called `problems` where the **label** is **\"PROBLEM\"** \n",
    "- Filter the problems to rows where: \n",
    "    - `is_family` is `True`, meaning that someone other than the patient experienced this\n",
    "    - The `text` attribute contains the word **\"cancer\"**. You can do this by using `problems[\"text\"].str.contains(\"keyword\")` in your filtering\n",
    "- Called this filtered DataFrame `fh_cancer`\n",
    "- Once you have created a filtered dataset, use the medspaCy widget to look at some of sentences containing family history of cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_cancer = problems[(problems[\"is_family\"] == ____)\n",
    "                    &\n",
    "                    (problems[____].str.contains(\"cancer\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrases extracted\n",
    "list(fh_cancer[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_docs = list(nlp.pipe(fh_cancer[\"sent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = MedspaCyVisualizerWidget(fh_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "# visualize_dep(fh_docs[idx])\n",
    "# visualize_ent(fh_docs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
